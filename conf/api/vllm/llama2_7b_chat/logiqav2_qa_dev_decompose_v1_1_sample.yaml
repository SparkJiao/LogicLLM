job:
  chdir: False
run:
  dir: outputs/${hydra.jon.config_path}/${hydra.job.config_name}/${now:%Y-%m-%d_%H-%M-%S}

train_file: logiqa-v2/train.txt
dev_file:
test_file: logiqa-v2/dev.txt

num_shot: 5

#output_file: openai-api-output/logiqav2-train-v1.1.100.qa.decompose.llama2.70b.chat.json
output_file: openai-api-output/logiqav2-dev-v1.1.full.qa.decompose.llama2.7b.chat.distil.sample10.json
lush_file: ${output_file}l

# Data loading
read_tensor:
  _target_: data.logic_form.ComposePromptGenerator
  read_func:
    _target_: data.readers.LogiQAReaderV2
    return_dict: True
    flat_options: True
  template_id: 3
  instruction:
  few_shot_prompt:
    _target_: data.logic_form.read_single_file
    file_path: data/prompts/logiqa/decomposition/gpt4/dev_10741_0.md
  compose_keys: [ "context", "question", "option_list" ]
  max_data_num: -1
  api_based: False
  service_based: True
  service_processor:
    _target_: data.collators.api.vllm.VLLMRequestGenerator
    api_url: http://localhost:8000/v1/completions
    n: 10
    max_tokens: 2560
    model: llama-2-7b-chat-distil
    temperature: 1.0

# Dataloader
num_workers: 64
prefetch_factor: 2

output_dir:

post_process:
  _target_: post_processors.openai_api_callback.OpenAICallBack
  output_file: ${output_file}
  answer_clean:
    _target_: post_processors.openai_api_callback.MCQAAnswerClean
    prompt: few-shot


# Training hyper-parameters
per_gpu_train_batch_size: 1
per_gpu_eval_batch_size: 1

ddp_eval: False
no_cuda: False
seed: 42
local_rank: -1

# Temporary variables
n_gpu: 1
device:
train_batch_size:
eval_batch_size:
world_size:
