train_micro_batch_size_per_gpu: ${per_gpu_train_batch_size}
gradient_accumulation_steps: ${gradient_accumulation_steps}
optimizer:
  type: AdamW
  params:
    lr: ${learning_rate}
    betas: [ 0.9, 0.99 ]
    eps: ${adam_epsilon}
    weight_decay: ${weight_decay}
scheduler:
  type: WarmupDecayLR
  params:
    total_num_steps:
    warmup_max_lr: ${learning_rate}
    warmup_num_steps:
    warmup_type: linear
gradient_clipping: ${max_grad_norm}
#  fp16:
#    enabled: ${fp16}
#    initial_scale_power: 12
bf16:
  enabled: ${fp16}
#  autotuning:
#    enabled: true
#    arg_mappings:
#      train_micro_batch_size_per_gpu: "per_gpu_train_batch_size"
#      gradient_accumulation_steps: "gradient_accumulation_steps"
#      zero_optimization: "ds_cfg.zero_optimization"
zero_optimization:
  stage: 1
  contiguous_gradients: True
  overlap_comm: True
  reduce_scatter: True
  reduce_bucket_size: 5e8
  allgather_bucket_size: 5e8
#    offload_optimizer:
#      device: cpu
#      pin_memory: True
#    offload_param:
#      device: cpu
#      pin_memory: True
#  activation_checkpointing:
#    partition_activations: True
#    cpu_checkpointing: True
#    contiguous_memory_optimization: False
#    number_checkpoints: False
#    synchronize_checkpoint_boundary: False
#    profile: False
steps_per_print: 1024